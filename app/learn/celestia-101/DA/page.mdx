---
description: Celestia's Data Availability layer and its key features.
---

# Celestia's data availability layer

Celestia is a data availability (DA) layer that provides a
scalable solution to the [data availability problem](https://coinmarketcap.com/academy/article/what-is-data-availability).
Due to the permissionless nature of the blockchain networks,
a DA layer must provide a mechanism for the execution and settlement
layers to check in a trust-minimized way whether transaction data is indeed available.

Two key features of Celestia's DA layer are [data availability sampling](https://blog.celestia.org/celestia-mvp-release-data-availability-sampling-light-clients)
(DAS) and [Namespaced Merkle trees](https://github.com/celestiaorg/nmt) (NMTs).
Both features are novel blockchain scaling solutions: DAS enables light
nodes to verify data availability without needing to download an entire block;
NMTs enable execution and settlement layers on Celestia to download transactions
that are only relevant to them.

## Data availability sampling (DAS)

In general, light nodes download only block headers that contain
commitments (_i.e._, Merkle roots) of the block data (_i.e._, the list of transactions).

To make DAS possible, Celestia uses a 2-dimensional Reed-Solomon
encoding scheme to encode the block data: every block data is split
into k × k shares, arranged in a k × k matrix, and extended with parity
data into a 2k × 2k extended matrix by applying multiple
times Reed-Solomon encoding.

Then, 4k separate Merkle roots are computed for the rows and columns
of the extended matrix; the Merkle root of these Merkle roots is used
as the block data commitment in the block header.

![2D Reed-Soloman (RS) Encoding](/img/learn/reed-solomon-encoding.png)

To verify that the data is available, Celestia light nodes are sampling
the 2k × 2k data shares.

Every light node randomly chooses a set of unique coordinates in the
extended matrix and queries bridge nodes for the data shares and the
corresponding Merkle proofs at those coordinates. If light nodes
receive a valid response for each sampling query, then there is a
[high probability guarantee](https://github.com/celestiaorg/celestia-node/issues/805#issuecomment-1150081075)
that the whole block's data is available.

Additionally, every received data share with a correct Merkle proof
is gossiped to the network. As a result, as long as the Celestia light
nodes are sampling together enough data shares (_i.e._, at least
k × k unique shares),
the full block can be recovered by honest bridge nodes.

For more details on DAS, take a look at the [original paper](https://arxiv.org/abs/1809.09044).

### Scalability

DAS enables Celestia to scale the DA layer. DAS can be performed by
resource-limited light nodes since each light node only samples a small
portion of the block data. The more light nodes there are in the network,
the more data they can collectively download and store.

This means that increasing the number of light nodes performing DAS allows
for larger blocks (_i.e._, with more transactions), while still keeping DAS
feasible for resource-limited light nodes. However, in order to validate
block headers, Celestia light nodes need to download the 4k intermediate
Merkle roots.

For a block data size of n² bytes, this means that every light node must
download O(n) bytes. Therefore, any improvement in the bandwidth capacity
of Celestia light nodes has a quadratic effect on the throughput of Celestia's
DA layer.

### Fraud proofs of incorrectly extended data

The requirement of downloading the 4k intermediate Merkle roots is a
consequence of using a 2-dimensional Reed-Solomon encoding scheme. Alternatively,
DAS could be designed with a standard (_i.e._, 1-dimensional) Reed-Solomon encoding,
where the original data is split into k shares and extended with k additional
shares of parity data. Since the block data commitment is the Merkle root of the
2k resulting data shares, light nodes no longer need to download O(n) bytes to
validate block headers.

The downside of the standard Reed-Solomon encoding is dealing with malicious
block producers that generate the extended data incorrectly.

This is possible as **Celestia does not require a majority of the consensus
(_i.e._, block producers) to be honest to guarantee data availability.**
Thus, if the extended data is invalid, the original data might not be
recoverable, even if the light nodes are sampling sufficient unique shares
(_i.e._, at least k for a standard encoding and k × k for a
2-dimensional encoding).

As a solution, _Fraud Proofs of Incorrectly Generated Extended Data_ enable
light nodes to reject blocks with invalid extended data. Such proofs require
reconstructing the encoding and verifying the mismatch. With standard Reed-Solomon
encoding, this entails downloading the original data, _i.e._, n² bytes.
Contrastingly, with 2-dimensional Reed-Solomon encoding, only O(n) bytes are
required as it is sufficient to verify only one row or one column of the
extended matrix.

## Namespaced Merkle trees (NMTs)

Celestia partitions the block data into multiple namespaces, one for
every application (e.g., rollup) using the DA layer. As a result, every
application needs to download only its own data and can ignore the data
of other applications.

For this to work, the DA layer must be able to prove that the provided
data is complete, _i.e._, all the data for a given namespace is returned.
To this end, Celestia is using Namespaced Merkle trees (NMTs).

An NMT is a Merkle tree with the leaves ordered by the namespace identifiers
and the hash function modified so that every node in the tree includes the
range of namespaces of all its descendants. The following figure shows an
example of an NMT with height three (_i.e._, eight data shares). The data is
partitioned into three namespaces.

![Namespaced Merkle Tree](/img/learn/nmt.png)

When an application requests the data for namespace 2, the DA layer must
provide the data shares `D3`, `D4`, `D5`, and `D6` and the nodes `N2`, `N8`
and `N7` as proof (note that the application already has the root `N14` from
the block header).

As a result, the application is able to check that the provided data is part
of the block data. Furthermore, the application can verify that all the data
for namespace 2 was provided. If the DA layer provides for example only the
data shares `D4` and `D5`, it must also provide nodes `N12` and `N11` as proofs.
However, the application can identify that the data is incomplete by checking
the namespace range of the two nodes, _i.e._, both `N12` and `N11` have descendants
part of namespace 2.

For more details on NMTs, refer to the [original paper](https://arxiv.org/abs/1905.09274).

## Building a PoS blockchain for DA

### Providing data availability

The Celestia DA layer consists of a PoS blockchain. Celestia is dubbing this
blockchain as the [celestia-app](https://github.com/celestiaorg/celestia-app),
an application that provides transactions to facilitate the DA layer and is built
using [Cosmos SDK](https://docs.cosmos.network/main). The following figure
shows the main components of celestia-app.

![Main components of celestia-app](/img/learn/celestia-app.png)

celestia-app is built on top of [celestia-core](https://github.com/celestiaorg/celestia-core),
a modified version of the [Tendermint consensus algorithm](https://arxiv.org/abs/1807.04938).
Among the more important changes to vanilla Tendermint, celestia-core:

- Enables the erasure coding of block data (using the 2-dimensional Reed-Solomon
  encoding scheme).
- Replaces the regular Merkle tree used by Tendermint to store block data with
  a [Namespaced Merkle tree](https://github.com/celestiaorg/nmt) that enables
  the above layers (_i.e._, execution and settlement) to only download the needed
  data (for more details, see the section below describing use cases).

For more details on the changes to Tendermint, take a look at the
[ADRs](https://github.com/celestiaorg/celestia-core/tree/v0.34.x-celestia/docs/celestia-architecture).
Notice that celestia-core nodes are still using the Tendermint p2p network.

Similarly to Tendermint, celestia-core is connected to the application layer
(_i.e._, the state machine) by [ABCI++](https://github.com/tendermint/tendermint/tree/master/spec/abci%2B%2B),
a major evolution of [ABCI](https://github.com/tendermint/tendermint/tree/master/spec/abci)
(Application Blockchain Interface).

The celestia-app state machine is necessary to execute the PoS logic and to
enable the governance of the DA layer.

However, the celestia-app is data-agnostic -- the state machine neither
validates nor stores the data that is made available by the celestia-app.

## The lifecycle of a celestia-app transaction

Users request the celestia-app to make data available by
sending `PayForBlobs` transactions. Every such transaction consists
of the identity of the sender, the data to be made available, also
referred to as the message, the data size, the namespace, and
a signature. Every block producer batches multiple `PayForBlobs`
transactions into a block.

Before proposing the block though, the producer passes it to the
state machine via ABCI++, where each `PayForBlobs` transaction is
split into a namespaced message (denoted by `Msg` in the figure below),
i.e., the data together with the namespace ID, and an executable
transaction (denoted by `e-Tx` in the figure below) that does not
contain the data, but only a commitment that can be used at a later
time to prove that the data was indeed made available.

Thus, the block data consists of data partitioned into namespaces
and executable transactions. Note that only these transactions are
executed by the Celestia state machine once the block is committed.

![Lifecycle of a celestia-app Transaction](/img/learn/tx-lifecycle.png)

Next, the block producer adds to the block header a commitment
of the block data. As
[described in the "Celestia's data availability layer" page](/learn/how-celestia-works/data-availability-layer.md),
the commitment is the Merkle root of the $4k$ intermediate Merkle roots
(i.e., one for each row and column of the extended matrix).
To compute this commitment, the block producer performs the following operations:

- It splits the executable transactions and the namespaced data
  into shares. Every share consists of some bytes prefixed by a
  namespace. To this end, the executable transactions are associated
  with a reserved namespace.
- It arranges these shares into a square matrix (row-wise). Note that
  the shares are padded to the next power of two. The outcome square
  of size $k \times k$ is referred to as the original data.
- It extends the original data to a $2k \times 2k$ square matrix using
  the 2-dimensional Reed-Solomon encoding scheme described above.
  The extended shares (i.e., containing erasure data) are associated
  with another reserved namespace.
- It computes a commitment for every row and column of the extended
  matrix using the NMTs described above.

Thus, the commitment of the block data is the root of a Merkle tree
with the leaves the roots of a forest of Namespaced Merkle subtrees,
one for every row and column of the extended matrix.

### Checking data availability

![DA network](/img/learn/consensus-da.png)

To enhance connectivity, the celestia-node augments the celestia-app
with a separate libp2p network, _i.e._, the so-called _DA network_,
that serves DAS requests.

Light nodes connect to a celestia-node in the DA network, listen to
extended block headers (i.e., the block headers together with the
relevant DA metadata, such as the $4k$ intermediate Merkle roots), and
perform DAS on the received headers (i.e., ask for random data shares).

Note that although it is recommended, performing DAS is optional -- light
nodes could just trust that the data corresponding to the commitments in
the block headers was indeed made available by the Celestia DA layer.
In addition, light nodes can also submit transactions to the celestia-app,
i.e., `PayForBlobs` transactions.

While performing DAS for a block header, every light node queries Celestia
Nodes for a number of random data shares from the extended matrix and the
corresponding Merkle proofs. If all the queries are successful, then the
light node accepts the block header as valid (from a DA perspective).

If at least one of the queries fails (i.e., either the data share is not
received or the Merkle proof is invalid), then the light node rejects the
block header and tries again later. The retrial is necessary to deal with
false negatives, i.e., block headers being rejected although the block
data is available. This may happen due to network congestion for example.

Alternatively, light nodes may accept a block header although the data
is not available, i.e., a _false positive_. This is possible since the
soundness property (i.e., if an honest light node accepts a block as available,
then at least one honest bridge node will eventually have the entire block data)
is probabilistically guaranteed (for more details, take a look at the
[original paper](https://arxiv.org/abs/1809.09044)).

By fine tuning Celestia's parameters (e.g., the number of data shares sampled
by each light node) the likelihood of false positives can be sufficiently
reduced such that block producers have no incentive to withhold the block data.

## Data retrievability and pruning

The purpose of data availability layers such as Celestia is to ensure
that block data is provably published, so that applications
and rollups can know what the state of their chain is, and store that data.
Once the data is published, data availability layers
[do not inherently guarantee that historical data will be permanently stored](https://notes.ethereum.org/@vbuterin/proto_danksharding_faq#If-data-is-deleted-after-30-days-how-would-users-access-older-blobs)
and remain retrievable.

In this document, we discuss the state of data retrievability and
pruning in Celestia, as well as some tips for rollup developers in
order to ensure that syncing new rollup nodes is possible.

### Data retrievability and pruning in celestia-node

As of version v0.13.0, celestia-node has implemented a light node
sampling window of 30 days, as specified in
[CIP-4](https://github.com/celestiaorg/CIPs/blob/main/cips/cip-004.md).
This means that once pruning is implemented,
light nodes will now only sample blocks within a 30-day
window instead of sampling all blocks from genesis. This change
introduces the concept of pruning to celestia-node, where data
outside of the 30-day window may not be stored by light nodes,
marking a significant update in how data retrievability and
storage are managed within the network
([v0.13.0 release notes](https://github.com/celestiaorg/celestia-node/releases/tag/v0.13.0)).

Data blobs older than the recency window will be pruned by default
on light nodes, after pruning is fully implemented,
but will continue to be stored by archival nodes that do not prune data. Light
nodes will be able to query historic blob data in namespaces from archival
nodes, as long as archival nodes exist on the public network.

Once pruning is fully implemented, light nodes will only perform data
availability sampling for blocks within the data recency window of 30 days.

### Suggested practices for rollups

Rollups may need to access historic data in order to allow new rollup nodes
to reconstruct the latest state by replaying historical blocks. Once data has
been published on Celestia and guaranteed to have been made available, rollups
and applications are responsible for storing their historical data.

While it is possible to continue to do this by using the `GetAll` API method in
celestia-node on historic blocks as long as archival nodes exist on the public
Celestia network, rollup developers should not rely on this as the only method
to access historical data, as archival nodes serving requests for historical
data for free is not guaranteed. Below are some other suggested methods to
access historical data.

- **Use professional archival node or data providers.** It is expected that
  professional infrastructure providers will provide paid access to archival
  nodes, where historical data can be retrieved, for example using the `GetAll`
  API method. Providers like QuickNode offer archival node services that maintain
  complete historical data, ensuring reliable access to past transactions and state.
  This provides better guarantees than solely relying on free archival nodes on the
  public Celestia network. For a list of available providers, see the
  [network pages](/how-to-guides/mainnet.md) page, and for specific archival
  node endpoints, refer to the [archival DA RPC endpoints](/how-to-guides/mainnet.md#archival-da-rpc-endpoints)
  section.

- **Share snapshots of rollup nodes.** Rollups could share snapshots of their
  data directories which can be downloaded manually by users bootstrapping new
  nodes. These snapshots could contain the latest state of the rollup, and/or
  all the historical blocks.
- **Add peer-to-peer support for historical block sync.** A less manual version
  of sharing snapshots, where rollup nodes could implement built-in support for
  block sync, where rollup nodes download historical block data from each other
  over a peer-to-peer network.
  - [**Namespace pinning.**](https://github.com/celestiaorg/celestia-node/issues/2830)
    In the future, celestia-node is expected to allow nodes to choose to "pin"
    data from selected namespaces that they wish to store and make available for
    other nodes. This will allow rollup nodes to be responsible for storing their
    data, without needing to implement their own peer-to-peer historical block
    sync mechanism.


## Monolithic vs. modular blockchains

Blockchains instantiate [replicated state machines](https://dl.acm.org/doi/abs/10.1145/98163.98167):
the nodes in a permissionless distributed network apply an ordered sequence
of deterministic transactions to an initial state resulting in a common
final state.

In other words, this means that nodes in a network all follow
the same set of rules (_i.e._, an ordered sequence of transactions) to go from a
starting point (_i.e._, an initial state) to an ending point
(_i.e._, a common final state). This process ensures that all
nodes in the network agree on the final state
of the blockchain, even though they operate independently.

This means blockchains
require the following four functions:

- **Execution** entails executing transactions that update the state correctly.
  Thus, execution must ensure that only valid transactions are executed, _i.e._,
  transactions that result in valid state machine transitions.
- **Settlement** entails an environment for execution layers to verify proofs,
  resolve fraud disputes, and bridge between other execution layers.
- **Consensus** entails agreeing on the order of the transactions.
- **Data Availability** (DA) entails making the transaction data available.
  Note that execution, settlement, and consensus require DA.

Traditional blockchains, _i.e._ _monolithic blockchains_, implement all four
functions together in a single base consensus layer. The problem with
monolithic blockchains is that the consensus layer must perform numerous
different tasks, and it cannot be optimized for only one of these functions.
As a result, the monolithic paradigm limits the throughput of the system.

![Modular VS Monolithic](/img/learn/monolithic-modular.png)

As a solution, modular blockchains decouple these functions among
multiple specialized layers as part of a modular stack. Due to the
flexibility that specialization provides, there are many possibilities
in which that stack can be arranged. For example, one such arrangement
is the separation of the four functions into three specialized layers.

The base layer consists of DA and consensus and thus, is referred to
as the Consensus and DA layer (or for brevity, the DA layer), while both
settlement and execution are moved on top in their own layers. As a result,
every layer can be specialized to optimally perform only its function, and thus,
increase the throughput of the system. Furthermore, this modular paradigm
enables multiple execution layers, _i.e._,
[rollups](https://vitalik.eth.limo/general/2021/01/05/rollup.html), to use the
same settlement and DA layers.
